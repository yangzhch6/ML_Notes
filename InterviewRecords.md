注：投论文错过了暑期实习，现在只能找些转正的日常

-------

# 华为诺亚（实习，应该是不给转正的）
## 专业面
都在问论文

## 代码面
一个二分法，一个是个数值计算问题:     
寻找$f(x) = 0$的解，思路也挺简单，就是找单调区间，然后接着二分缩小范围到可接受精度内。

## 实习计划
其实就是跟指定的mentor聊下实习要干嘛

比较搞的是，实习计划都定了结果告诉爷没坑位了


-------

# 字节推荐（日常实习，给转正机会）
## 一面

1. Dropout参数是什么？    
    我说是p，他说你再想想....不是很懂...

2. 后面问的问题比较简单，都忘了

3. BN和Dropout可不可以一起用?    
    我说可以（当时只说了可以，但没有说为什么可以，参照[Tricks](8-Tricks.md)中的BN一节）

代码题：n个数字，执行n-1次操作，尝试让最后得到的数值最大。操作分成两种：
1. 删除俩相同的数，加入两者之和
2. 删除俩不同的，加入最大的那个

    我用最小堆做的，叠加贪心。思路是先根据当前数组建立优先队列。从小到大进行取值，如果上面俩值是相同的，就执行操作1然后把相加后的数加入优先队列；如果上面俩值不同，就执行操作2，更大的数重新入列。直到只剩下一个数。

    （面试前要看看数据结构基础啊，最小堆咋实现的我都忘了，后面被问到有点尴尬，我当时说我只记得是个满二叉树，然后在数组上做操作）

## 二面
专业问题：    
都比较简单，没啥印象了

算法题：
1. 实现一下LRU    
    LRU算法是选择最近最久未使用的页面予以淘汰

    本来忘了这算法是啥，被她简单提醒后很快写出来了

2. 一道快速幂，要你求a的b次方的最后三位数，其中a是小于1000的数字。

    快速幂忘了，因此我第一时间写的方法是，记录下$a^{1-1001}$的末三位，一定会有重复的，可以绕成一个圈。所以复杂度也很低。（这种题还是最好快速幂啊）

本来面试的剪映的推荐，面试官说我nlp的更适合去啥开放平台做推荐，后来我就来面开放平台了。（其实当时我啥推荐都不会）

## 三面
1. 上来先问我推荐系统咋设计     
    没学过，讲道理确实不太清楚。   
    他说我不清楚推荐系统干嘛来做这个？我说因为这歪意儿比较务实。    
    他看到我说搞过一段时间GNN，问我GNN怎么用到推荐里，我开始乱掰。    
    （这一坨大概问了20min...）

2. **Logic Regress为什么用sigmoid？**    
    很尴尬，去年还看过这个，后来忘了

3. 如何解决样本不均衡？    
    采样、甚至可以GAN生成、loss里加weight、focal loss

4. 用focal loss的时候，他发现概率的分布有偏差。比如点击率预测，如果ground truth里平均是1%，用CE来预测，打印出预测概率p并计算平均值；再用focal loss来预测，打印出预测概率p并计算平均值。他发现focal loss预测的p相比于交叉熵，距离1%这一 ground truth 偏移的更大。问为什么？       
    一开始没理解他的问题，交流之后才懂。我的回答是：交叉熵损失函数的目标，本来就是缩小预测概率$\hat p$和真实概率分布$p$的差距。而focal loss改变CE的公式后，其就不再是拟合的原来真是概率$p$的分布了。
    （面试官说这个解释他能接受，不清楚正确答案是不是我这个）

代码题：比较简单，忘了是啥了。

PS：这大概是目前为止最尬的面试，推荐确实没学


## 四面
上来就是一个算法题：给定一个矩阵N x M，要求找到相邻的严格递增序列的最长长度，相邻指上下左右四个方向。（也即该严格递增序列必须在上下左右四个方向相连接）

我开一个N x M数组记录每个节点对应的，开节点开始的严格递增序列的最长的长度。并开一个数组记录当前序列是否已经遍历一遍了。然后对矩阵的每一个节点进行遍历：
1. 如果遍历过，则跳过
2. 如果没遍历过则进行dfs（dfs中如果遍历到访问过的节点，直接返回当前节点对应的最长严格递增序列长度即可，如果没访问到就继续dfs）    

最后返回最长的值即可，复杂度为O(N x M)

后面问的问题：    

1. 为什么神经网络权重不可以初始化为0？

2. **为什么回归的时候MSE做损失函数不可以用Sigmoid？**    
    没答上来，我说sigmoid是大于0的，当然target可以放缩到0-1之间。但除此之外，sigmoid对接近0和1的点，其梯度很小，处于梯度不饱和区域。    
    面试官说这题答案应该是sigmoid+MSE不是凸函数。 
    <ins>这个我有点疑惑，没有搞懂</ins>

3. 手推逻辑回归反向传播公式   
    我觉得的我没错，他说我还是错了。后来下来对了一遍，nmd有一步少抄了一个负号...

4. 老问题，为啥做nlp的要来做推荐

5. 问了个推荐的问题：给定u_id, video_id, video_title, label(是否点击)。 每一行数据都是推给用户的。问怎么设计这个推荐系统：       
    不懂推荐系统就强答了。我说做一个u_id对应一个embedding layer当作u_id的embedding。video的embedding有两个，因为title只是标题内容，但标题相似的视频可能代表不同内容，因此    
    $v \underline{~} embedd = embedding \underline{~} layer(v_id) + Bert(v \underline{~} title)$      
    然后两embedding做attention或者其他操作，甚至更进一步的可以用contrastive loss去训练他们的embedding。
    （纯脑洞呀...）


-------
# 幻方 （正式职位）
## 笔试
还挺难的，除去几个选择题。里面代码题目难得有俩：

1. 实现一个BNF文法解释器    
    忘光了，不会写

2. 实现一个LZ77/LZ78压缩算法    
    这个是填代码的，根据他写的压缩函数，你来完成解压函数。所以难度主要在看懂别人的代码。几乎没有注释，太菜了看了半天没整出来。

## 面试
前一个小时问问题：
1. 问了下学校论文

2. 讲讲Transformer和RNN，LSTM区别

3. 为什么序列任务里，LSTM比transformer好？     
    我有点懵？我说transformer里的self attention是忽略了位置信息的，加了position embedding或者在attention score那里硬编码进位置信息，才能把让模型有能力区分不同位置。因此，只有加了位置信息的transformer才能比LSTM强。而LSTM是通过几个门来控制遗忘的，从模型角度，transformer表示能力更强，从训练的角度，同参数量的LSTM比transformer慢，因此绝大多数情况一定是transformer强于LSTM。

    **他说（大致意思）：“其实我想听到的答案是，语言里的位置信息不够重要，所以transformer处理的不错，但一些金融数据位置信息很重要，因此LSTM更好。”**    ~~~蛤？？

4. 多卡训练怎么做？     
    模型并行：模型太大gpu太烂，不同gpu放不同模型    
    数据并行：GPU里放相同模型，传入不同数据。按照GPU间通信方式可分为树状通信和环状通信（环状通信更优）

后一个小时写代码+问问题
1. IOU实现一下     
    一开始有点蠢，在那分类各种情况，其实只需要用min、max找到交叉的左上右下两个点就好

2. 合并两个链表还是啥，忘了，这题反正挺简单

3. 讲讲Transformer和RNN，LSTM区别？    
    又来？

过了几天通知，面试gg


-------
# CUHK提前批
时间很短，大概15min，问的问题也很简单，但是要用英文讲。             

0. 一分钟介绍下自己     
    他问我准备好没，我神经反射蹦了个“好”出来，这人愣了两秒哈哈哈....       
    两秒后我说“OK fine”

1. 介绍下二叉搜索树    
英文不好，讲的很磕巴  

2. 二叉搜索树极端情况是什么，有什么解决办法？          
平衡二叉树，讲了下四个旋转，很磕巴

3. 0-99一共100个数，现在有一个数组，其中缺少了0-99其中的一个数，如何找出来？            
0-99相加减去sum(array)

4. 上面这个问题，如果缺少两个数怎么找？          
HashTable

5. 给一个N>0，再给一个S=0，每次S可执行两种操作，S+=1或者S+=2，请问让S=N有多少种方法？       
我说dynamic programing，然后给了个状态转移方程


周五面试，周一邮件说通过了