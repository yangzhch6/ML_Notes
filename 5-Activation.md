## 1. Sigmoid

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

优点：...

缺点：
1. 存在梯度弥散(输入值过大/过小时，激活函数计算的梯度很小)
2. 不关于原点对称
3. exp的计算量相对较大

-----
## 2. Tanh

$$
Tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

优点：
1. 解决了Sigmoid原点不对称问题
2. 比Sigmoid更快收敛(?)

缺点：
1. 存在梯度弥散(输入值过大/过小时，激活函数计算的梯度很小)

-----
## 3. ReLU

$$
f_{\rm ReLU} = max(0, x)
$$

优点:
1. 解决了部分梯度弥散问题
2. 收敛速度更快
3. 计算量小

缺点：
1. 输入值小于0时，相当于神经元死亡，因此依然存在梯度弥散。

-----
## 4. Leaky ReLU

$$
f(x) = 
\left\{
\begin{array}{lr}
\alpha x , &x < 0\\
x, &x \geq 0
\end{array}
\right.
$$

优点:
1. 解决了梯度弥散问题(解决了ReLU神经元死亡问题)
2. 收敛速度更快
3. 计算量小

缺点：...

-----
相关链接：
1. [常用激活函数的比较](https://zhuanlan.zhihu.com/p/32610035)
