## 1. 如何提升泛化性？
数据角度：
1. 更多的数据（收集、采样、生成）
2. 数据增强：图片可以放大缩小，翻转等等。文本可以做同义词替换，随机删除，随机交换位置，回译等等
3. 更好的特征：组合特征，人工选择特征（神经网络受不相关数据的影响很小。它们会对此赋予一个趋近于0的权重，几乎忽略此特征对预测值的贡献。）

模型角度：
1. [小批量训练](https://zhuanlan.zhihu.com/p/455324267)
2. Normalization
3. Regularization
4. Bagging
5. Dropout (可视为Bagging的一种形式)

## 2. 怎么处理过拟合？
跟提升泛化性类似，可在其基础上增加下面的方法：
1. 减少模型参数量（<ins>个人认为：在缓解过拟合和提升泛化性方面，控制模型参数量是个需要均衡的问题。有时提升泛化性要增加模型参数，有时缓解过拟合要减小参数。</ins>）


## 3. 怎么处理欠拟合？
数据角度：
1. 数据未做归一化处理      
    我们需要对数据进行归一化操作的原因，主要是我们一般假设输入和输出数据都是服从均值为 0，标准差为 1 的正态分布。这种假设在深度学习理论中非常常见，从权重初始化，到激活函数，再到对训练网络的优化算法。

2. 特征过少，增加新特征

模型角度：
1. 模型复杂度太低


## 4. 样本不均衡怎么办？
数据角度：
1. 过采样/欠采样
2. 扩大数据集（数据增强）

模型角度：
1. loss加上weight
2. focal loss（难例挖掘）
3. 采样+集成学习    
    这部分还没看明白: [一文解决样本不均衡](https://jishuin.proginn.com/p/763bfbd6f706) 


## 5. Gradient Vanish & Explode
二者都是由于网络层数加深，雅可比矩阵(都大于1或小于1)连乘导致的问题

### 1.1 Gradient Explode (梯度爆炸) 解决方案
1. 剪裁（**需要注意的是该方法很难解决梯度消失，主要是面向梯度爆炸提出的**）
2. Normalization
3. pre-training+fine-tunning
4. weithts regularization（权重正则化）防止权重过大，从而导致连乘时梯度爆炸

### 1.2 Gradient Vanish
1. LSTM
2. ReLU (<ins>存在疑惑：对RNN难以起到作用？</ins>)
3. Normalization
4. 残差 (<ins>存在疑惑：从推导来看解决的是梯度消失，无法解决梯度爆炸</ins>)
5. pre-training+fine-tunning